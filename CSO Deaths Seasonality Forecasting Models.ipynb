{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1129df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import datetime\n",
    "from pandas import to_datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import mitosheet as sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50218040",
   "metadata": {},
   "source": [
    "## Import and Format Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8e24c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Modelling Data.csv')#read modelling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c895f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head() #inspect top 5 rows of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32abb82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail() #inspect bottom 5 rows of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aa825e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info() #get information on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686f18d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Q_num'] = (df.Quarter.astype(str)).str[5] \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7289cae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Quarter'] = pd.to_datetime(df['Quarter'])#format Quarter as datetime instead of object. This is needed in order to plot dataset as a time series.\n",
    "df.info() #get info on reformatted dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7865d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()#inspect top 5 rows of reformatted dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04451f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('Quarter') #set 'Quarter' as the dataframe index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6dbfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head() #inspect top 5 rows of reformatted dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8283f456",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_years = [4,3,2] #define three test year periods: 4 years, 3 years and 2 years"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f3c967",
   "metadata": {},
   "source": [
    "## SARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9d0d35",
   "metadata": {},
   "source": [
    "### Run model for each set of training data and each train/test period. Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabdb7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model we are using\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47253c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"SARIMA\" #define model name\n",
    "#write for loop to iterate over the 3 train/test periods\n",
    "for i in test_years:\n",
    "    training_data = df[['VALUE']].iloc[:-i*4] #set the training data equal to df_SV less the last 16 (4*4), 12 (3*4) and 8 (2*4) quarters\n",
    "    testing_data = df[['VALUE']].tail(i*4) #set testing data equal to the last 16 (4*4), 12 (3*4) and 8 (2*4) quarters of df_SV\n",
    "    y=training_data\n",
    "    SARIMAXmodel = SARIMAX(y, order = (1,1,1), seasonal_order=(1,1,1,12)) #'12' used as the seasonality in the time series is repeated every calendar year\n",
    "    SARIMAXmodel = SARIMAXmodel.fit()\n",
    "    predictions = SARIMAXmodel.get_forecast(len(testing_data.index))\n",
    "    predictions = predictions.conf_int(alpha = 0.05) \n",
    "    predictions[\"Predictions\"] = SARIMAXmodel.predict(start = predictions.index[0],\n",
    "                                            end = predictions.index[-1])\n",
    "    predictions.index = testing_data.index\n",
    "    y_pred = pd.DataFrame(predictions[\"Predictions\"]) #'predictions' is an array - convert to a dataframe\n",
    "    exec(f'{model_name}_Predictions_{i}y = y_pred')\n",
    "    exec(f'{model_name}_MAE_{i}y = mean_absolute_error(testing_data.values, y_pred[\"Predictions\"])')#calculate and save MAE of the model predictions for each train/test period\n",
    "    exec(f'{model_name}_RMSE_{i}y= np.sqrt(mean_squared_error(testing_data.values, y_pred[\"Predictions\"]))')#calculate and save RMSE of the model predictions for each train/test period"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3e76a4",
   "metadata": {},
   "source": [
    "## Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a56c2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1e8c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_array = np.array(df.Q_num) #convert Q_num to an array\n",
    "df_array = df_array.reshape(-1, 1) #reshape array so that it can be processed by the Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652b8106",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_array #inspect the array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e4cf69",
   "metadata": {},
   "source": [
    "### Run model for each set of training data and each train/test period. Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7756d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9dca39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"RF\" #define model name\n",
    "#write for loop to iterate over the 3 training/testing periods\n",
    "for i in test_years:\n",
    "    train_features = df_array[:-i*4]\n",
    "    test_features = df_array[-i*4:len(df_array)]\n",
    "    labels = np.array(df['VALUE'])\n",
    "    train_labels = labels[:-i*4]\n",
    "    test_labels =  labels[-i*4:len(df_array)]\n",
    "    # Instantiate model with 1000 decision trees\n",
    "    rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "    # Train the model on training data\n",
    "    rf.fit(train_features, train_labels);\n",
    "    # Use the forest's predict method on the test data\n",
    "    predictions = rf.predict(test_features)  \n",
    "    training_data = df[['VALUE']].iloc[:-i*4] #for plotting only, set the training data equal to the full df_SV less the last 16 (4*4), 12 (3*4) and 8 (2*4) quarters\n",
    "    testing_data = df[['VALUE']].tail(i*4) #for plotting only, set testing data equal to the last 16 (4*4), 12 (3*4) and 8 (2*4) quarters of df_SV\n",
    "    y_pred = pd.DataFrame(predictions, columns=['Predictions']) #'predictions' is an array - convert to a dataframe\n",
    "    y_pred.index = testing_data.index\n",
    "    exec(f'{model_name}_Predictions_{i}y = y_pred')\n",
    "    exec(f'{model_name}_MAE_{i}y = mean_absolute_error(testing_data.values, y_pred[\"Predictions\"])')#calculate and save MAE of the model predictions for each train/test period\n",
    "    exec(f'{model_name}_RMSE_{i}y= np.sqrt(mean_squared_error(testing_data.values, y_pred[\"Predictions\"]))')#calculate and save RMSE of the model predictions for each train/test period "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5328362f",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fa2f0e",
   "metadata": {},
   "source": [
    "### Format data for LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7b5642",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = df[['VALUE']].iloc[:, 0].values #Transform dataframe to an array as the keras module of TensorFlow only accepts NumPy arrays as parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d8b309",
   "metadata": {},
   "source": [
    "### Run model for each set of training data and each train/test period. Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad157c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import classes from TensorFlow that are needed to build the RNN \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dropout\n",
    "scaler = MinMaxScaler() #define MinMaxScaler() as 'scaler'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593888bf",
   "metadata": {},
   "source": [
    "#### Specify 1 Timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbe0acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"LSTM_1ts\" #define model name\n",
    "timesteps  = 1  #specify 1 timestep - this means that the model uses the last timestep to predict the next timestep\n",
    "#write for loop to iterate over the 3 training/testing periods\n",
    "for i in test_years: \n",
    "    training_data = array[:-i*4] #set the training data as an array equal to the full Seasonal Variation dataset less the last 16 (4*4), 12 (3*4) and 8 (2*4) quarters\n",
    "    testing_data_ = array[-i*4:len(array)] #set testing data as an array equal to the last 16 (4*4), 12 (3*4) and 8 (2*4) quarters\n",
    "    scaled_training_data = scaler.fit_transform(training_data.reshape(-1, 1)) #Scale testing data so that it fits between -1 and +1\n",
    "    scaled_testing_data_ = scaler.fit_transform(testing_data_.reshape(-1, 1)) #Scale testing data so that it fits between -1 and +1\n",
    "    x_training_data = [] #Create empty list called x_training_data\n",
    "    y_training_data =[] #Create empty list called y_training data\n",
    "    for j in range(timesteps, len(scaled_training_data)): #Specify 4 timesteps. For every quarter's seasonal variation the model predicts, it will use 4 previous quarters pricing to determine its output.  \n",
    "        x_training_data.append(scaled_training_data[j-timesteps:j, 0]) #Use for loop to populate actual data into the  and y_training_data lists\n",
    "        y_training_data.append(scaled_training_data[j, 0])\n",
    "    x_training_data = np.array(x_training_data) #TensorFlow is designed to work with arrays so we need to transform the two Python lists we created in the previous step into NumPy arrays.\n",
    "    y_training_data = np.array(y_training_data)\n",
    "    x_training_data = np.reshape(x_training_data, (x_training_data.shape[0], #Reshape the training data one more time because the RNN layer available in TensorFlow only accepts data in a very specific format.  \n",
    "    x_training_data.shape[1],1))\n",
    "    \n",
    "    rnn = Sequential() #Initialise an object from TensorFlows Sequential class. This class is designed to build neural networks by adding sequences of layers over time.\n",
    "    rnn.add(LSTM(units = 50, return_sequences = True, input_shape =(x_training_data.shape[1], 1))) #Add an LSTM layer first. Specify 3 arguments: Units = 50, return_sequences = True, input_shape = 4 timesteps and  1 predictor \n",
    "    rnn.add(Dropout(0.2)) #Dropout regularisation is a technique used to avoid overfitting when training neural networks. Assume a dropout rate of 20%.\n",
    "    rnn.add(LSTM(units = 50, return_sequences = True)) #Add three more LSTM layers\n",
    "    rnn.add(Dropout(0.2))\n",
    "    rnn.add(LSTM(units = 50, return_sequences = True))\n",
    "    rnn.add(Dropout(0.2))\n",
    "    rnn.add(LSTM(units = 50)) #Remove return_sequences = True as this is the last LSTM layer\n",
    "    rnn.add(Dropout(0.2))\n",
    "    rnn.add(Dense(units = 1)) #Add output layer. Units = 1 as we want to predict the next quarter's seasonal variation\n",
    "    rnn.compile(optimizer = 'adam', loss = 'mean_absolute_error') #Specify Adam optimiser. Since we are predicting a continuous variable we can use Mean Squared Error as our loss parameter.\n",
    "    rnn.fit(x_training_data, y_training_data, epochs = 100, batch_size = 32)  #Train the RNN on our training data ( and y_training_data). Epochs = number of iterations I want the RNN to train on. Batch size = size of batches RNN will be trained on through each epoch.\n",
    "    \n",
    "    x_test_data = df[['VALUE']][len(df[['VALUE']])-i*4-timesteps:] #we need 4 quarters (testing data) set + 1 timestep =  5 quarters\n",
    "    x_test_data = x_test_data.iloc[:,0].values\n",
    "    x_test_data.shape #x_test_data should have 5 rows and no columns\n",
    "    x_test_data = np.reshape(x_test_data, (-1, 1)) #reshape NumPy array to make it suitable for the predict method\n",
    "    x_test_data = scaler.transform(x_test_data) #Scale test data so we can use model to make predictions. This is because we want to transform the test data according to the fit generated from\n",
    "#the entire training data set. This means that the transformation that is applied to the test data will be the same as the one applied to the training data - which is necessary for our recurrent neural network to make\n",
    "#accurate predictions\n",
    "    final_x_test_data = []                             \n",
    "    for j in range(timesteps, len(x_test_data)):\n",
    "        final_x_test_data.append(x_test_data[j-timesteps:j, 0])\n",
    "    final_x_test_data = np.array(final_x_test_data)\n",
    "    final_x_test_data = np.reshape(final_x_test_data, (final_x_test_data.shape[0],final_x_test_data.shape[1],1)) #reshape final test data to meet TensorFlow standards\n",
    "    predictions = rnn.predict(final_x_test_data) #Pass final_x_test_data into the predict() method called on the run object and store in a variable called predictions. \n",
    "    unscaled_predictions = scaler.inverse_transform(predictions) #unscale predictions data\n",
    "    training_data = df[['VALUE']].iloc[:-i*4] #for plotting only, set the training data equal to the full df_SV less the last 16 (4*4), 12 (3*4) and 8 (2*4) quarters\n",
    "    testing_data = df[['VALUE']].tail(i*4) #for plotting only, set testing data equal to the last 16 (4*4), 12 (3*4) and 8 (2*4) quarters of df_SV\n",
    "    y_pred = pd.DataFrame(unscaled_predictions, columns=['Predictions']) #'unscaled_predictions' is an array - convert to a dataframe\n",
    "    y_pred.index = testing_data.index\n",
    "    exec(f'{model_name}_Predictions_{i}y = y_pred')\n",
    "    exec(f'{model_name}_MAE_{i}y = mean_absolute_error(testing_data.values, y_pred[\"Predictions\"])')#calculate and save MAE of the model predictions for each train/test period\n",
    "    exec(f'{model_name}_RMSE_{i}y= np.sqrt(mean_squared_error(testing_data.values, y_pred[\"Predictions\"]))')#calculate and save RMSE of the model predictions for each train/test period"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ab2601",
   "metadata": {},
   "source": [
    "#### Specify 4 Timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9423c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = \"LSTM_4ts\" #define model name\n",
    "timesteps  = 4 #specify 1 timestep - this means that the model uses the last 4 timesteps to predict the next 4 timesteps\n",
    "#write for loop to iterate over the 3 training/testing periods\n",
    "for i in test_years: \n",
    "    training_data = array[:-i*4] #set the training data as an array equal to the full Seasonal Variation dataset less the last 16 (4*4), 12 (3*4) and 8 (2*4) quarters\n",
    "    testing_data_ = array[-i*4:len(array)] #set testing data as an array equal to the last 16 (4*4), 12 (3*4) and 8 (2*4) quarters\n",
    "    scaled_training_data = scaler.fit_transform(training_data.reshape(-1, 1)) #Scale testing data so that it fits between -1 and +1\n",
    "    scaled_testing_data_ = scaler.fit_transform(testing_data_.reshape(-1, 1)) #Scale testing data so that it fits between -1 and +1\n",
    "    x_training_data = [] #Create empty list called x_training_data\n",
    "    y_training_data =[] #Create empty list called y_training data\n",
    "    for j in range(timesteps, len(scaled_training_data)): #Specify 4 timesteps. For every quarter's seasonal variation the model predicts, it will use 4 previous quarters pricing to determine its output.  \n",
    "        x_training_data.append(scaled_training_data[j-timesteps:j, 0]) #Use for loop to populate actual data into the  and y_training_data lists\n",
    "        y_training_data.append(scaled_training_data[j, 0])\n",
    "    x_training_data = np.array(x_training_data) #TensorFlow is designed to work with arrays so we need to transform the two Python lists we created in the previous step into NumPy arrays.\n",
    "    y_training_data = np.array(y_training_data)\n",
    "    x_training_data = np.reshape(x_training_data, (x_training_data.shape[0], #Reshape the training data one more time because the RNN layer available in TensorFlow only accepts data in a very specific format.  \n",
    "    x_training_data.shape[1],1))\n",
    "    \n",
    "    rnn = Sequential() #Initialise an object from TensorFlows Sequential class. This class is designed to build neural networks by adding sequences of layers over time.\n",
    "    rnn.add(LSTM(units = 50, return_sequences = True, input_shape =(x_training_data.shape[1], 1))) #Add an LSTM layer first. Specify 3 arguments: Units = 50, return_sequences = True, input_shape = 4 timesteps and  1 predictor \n",
    "    rnn.add(Dropout(0.2)) #Dropout regularisation is a technique used to avoid overfitting when training neural networks. Assume a dropout rate of 20%.\n",
    "    rnn.add(LSTM(units = 50, return_sequences = True)) #Add three more LSTM layers\n",
    "    rnn.add(Dropout(0.2))\n",
    "    rnn.add(LSTM(units = 50, return_sequences = True))\n",
    "    rnn.add(Dropout(0.2))\n",
    "    rnn.add(LSTM(units = 50)) #Remove return_sequences = True as this is the last LSTM layer\n",
    "    rnn.add(Dropout(0.2))\n",
    "    rnn.add(Dense(units = 1)) #Add output layer. Units = 1 as we want to predict the next quarter's seasonal variation\n",
    "    rnn.compile(optimizer = 'adam', loss = 'mean_absolute_error') #Specify Adam optimiser. Since we are predicting a continuous variable we can use Mean Absolute Error as our loss parameter.\n",
    "    rnn.fit(x_training_data, y_training_data, epochs = 100, batch_size = 32)  #Train the RNN on our training data ( and y_training_data). Epochs = number of iterations I want the RNN to train on. Batch size = size of batches RNN will be trained on through each epoch.\n",
    "    \n",
    "    x_test_data = df[['VALUE']][len(df[['VALUE']])-i*4-timesteps:] #we need 4 quarters (testing data) set + 1 timestep =  5 quarters\n",
    "    x_test_data = x_test_data.iloc[:,0].values\n",
    "    x_test_data.shape #x_test_data should have 5 rows and no columns\n",
    "    x_test_data = np.reshape(x_test_data, (-1, 1)) #reshape NumPy array to make it suitable for the predict method\n",
    "    x_test_data = scaler.transform(x_test_data) #Scale test data so we can use model to make predictions. This is because we want to transform the test data according to the fit generated from\n",
    "#the entire training data set. This means that the transformation that is applied to the test data will be the same as the one applied to the training data - which is necessary for our recurrent neural network to make\n",
    "#accurate predictions\n",
    "    final_x_test_data = []                             \n",
    "    for j in range(timesteps, len(x_test_data)):\n",
    "        final_x_test_data.append(x_test_data[j-timesteps:j, 0])\n",
    "    final_x_test_data = np.array(final_x_test_data)\n",
    "    final_x_test_data = np.reshape(final_x_test_data, (final_x_test_data.shape[0],final_x_test_data.shape[1],1)) #reshape final test data to meet TensorFlow standards\n",
    "    predictions = rnn.predict(final_x_test_data) #Pass final_x_test_data into the predict() method called on the run object and store in a variable called predictions. \n",
    "    unscaled_predictions = scaler.inverse_transform(predictions) #unscale predictions data\n",
    "    training_data = df[['VALUE']].iloc[:-i*4] #for plotting only, set the training data equal to the full df_SV less the last 16 (4*4), 12 (3*4) and 8 (2*4) quarters\n",
    "    testing_data = df[['VALUE']].tail(i*4) #for plotting only, set testing data equal to the last 16 (4*4), 12 (3*4) and 8 (2*4) quarters of df_SV\n",
    "    y_pred = pd.DataFrame(unscaled_predictions, columns=['Predictions']) #'unscaled_predictions' is an array - convert to a dataframe\n",
    "    y_pred.index = testing_data.index\n",
    "    exec(f'{model_name}_Predictions_{i}y = y_pred')\n",
    "    exec(f'{model_name}_MAE_{i}y = mean_absolute_error(testing_data.values, y_pred[\"Predictions\"])')#calculate and save MAE of the model predictions for each train/test period\n",
    "    exec(f'{model_name}_RMSE_{i}y= np.sqrt(mean_squared_error(testing_data.values, y_pred[\"Predictions\"]))')#calculate and save RMSE of the model predictions for each train/test period"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a58f32",
   "metadata": {},
   "source": [
    "## Prophet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0177b7f4",
   "metadata": {},
   "source": [
    "### Format data for Prophet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353a9f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Prophet = df[['VALUE']] #create new dataframe called df_Prophet \n",
    "df_Prophet.reset_index(inplace=True) #reset the index\n",
    "df_Prophet.columns = ['ds','y'] #rename the columns to 'ds' and 'y' in order for the Prophet model to process the input data\n",
    "df_Prophet['ds'] = to_datetime(df_Prophet['ds']) #convert the 'ds' column to datetime format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1408750",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Prophet.head() #inspect the first 5 rows of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b4429a",
   "metadata": {},
   "source": [
    "### Run model for each set of training data and each train/test period. Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83bba4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model we are using\n",
    "from prophet import Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfbd47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Prophet\" #define model name\n",
    "#run for loop to iterate over the 3 different test year periods\n",
    "for i in test_years: \n",
    "    df_train = df_Prophet[:-i*4]\n",
    "    model = Prophet()\n",
    "    model.fit(df_train)\n",
    "    future = df_Prophet[-i*4:len(df_Prophet)]\n",
    "    future.columns = ['ds','y']\n",
    "    forecast = model.predict(future)\n",
    "    print(forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']])  # summarise the forecast\n",
    "    model.plot(forecast) # plot forecast\n",
    "    y_pred = forecast[['ds','yhat']]\n",
    "    y_pred =  y_pred.set_index('ds')\n",
    "    training_data = df_train.set_index('ds') #for plotting, set the 'ds' column of the training data as the index\n",
    "    testing_data_ = df_Prophet[-i*4:len(df_Prophet)] #for plotting only, set testing data equal to the last 16 (4*4), 12 (3*4) and 8 (2*4) quarters\n",
    "    testing_data_ = testing_data_.set_index('ds') #for plotting, set the 'ds' column of the testing data to the index\n",
    "    y_pred = y_pred.rename(columns={'yhat': 'Predictions'})\n",
    "    exec(f'{model_name}_Predictions_{i}y = y_pred')\n",
    "    exec(f'{model_name}_MAE_{i}y = mean_absolute_error(testing_data_.values, y_pred[\"Predictions\"])') #calculate and save MAE of the model predictions for each train/test period\n",
    "    exec(f'{model_name}_RMSE_{i}y= np.sqrt(mean_squared_error(testing_data_.values, y_pred[\"Predictions\"]))') #calculate and save RMSE of the model predictions for each train/test period"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaba3b7",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f537832b",
   "metadata": {},
   "source": [
    "### Rank Model Results by MAE & RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d584e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_MAE = pd.DataFrame() #create an empty DataFrame\n",
    "# append columns to an empty DataFrame\n",
    "df_MAE['Model'] = ['SARIMA','Random Forest','LSTM - 1 Timestep', 'LSTM - 4 Timesteps','Prophet'] #define column names\n",
    "df_MAE['MAE (4 Year Testing Period)'] = [SARIMA_MAE_4y,RF_MAE_4y,LSTM_1ts_MAE_4y,LSTM_4ts_MAE_4y,Prophet_MAE_4y] #populate 'MAE 4 year' column with MAE_4y result for each model\n",
    "df_MAE['MAE (3 Year Testing Period)'] = [SARIMA_MAE_3y,RF_MAE_3y,LSTM_1ts_MAE_3y,LSTM_4ts_MAE_3y,Prophet_MAE_3y] #populate 'MAE 3 year' column with MAE_4y result for each model\n",
    "df_MAE['MAE (2 Year Testing Period)'] = [SARIMA_MAE_2y,RF_MAE_2y,LSTM_1ts_MAE_2y,LSTM_4ts_MAE_2y,Prophet_MAE_2y] #populate 'MAE 2 year' column with MAE_4y result for each model\n",
    "df_MAE['MAE (Average)'] = (df_MAE['MAE (4 Year Testing Period)'] + df_MAE['MAE (3 Year Testing Period)'] + df_MAE['MAE (2 Year Testing Period)'])/3 #populate 'MAE Average'column with average MAE across the 3 training periods for each model\n",
    "df_MAE = df_MAE.set_index('Model') #set 'Model' column equal to the dataframe index\n",
    "df_MAE.sort_values(by='MAE (Average)', ascending=True) #sort MAE results in ascending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7fb644",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RMSE = pd.DataFrame() #create an empty DataFrame\n",
    "# append columns to an empty DataFrame\n",
    "df_RMSE['Model'] = ['SARIMA','Random Forest', 'LSTM - 1 Timestep','LSTM - 4 Timesteps','Prophet'] #define column names\n",
    "df_RMSE['RMSE (4 Year Testing Period)'] = [SARIMA_RMSE_4y,RF_RMSE_4y,LSTM_1ts_RMSE_4y,LSTM_4ts_RMSE_4y,Prophet_RMSE_4y] #populate 'RMSE 4 year' column with RMSE_4y result for each model\n",
    "df_RMSE['RMSE (3 Year Testing Period)'] = [SARIMA_RMSE_3y,RF_RMSE_3y,LSTM_1ts_RMSE_3y,LSTM_4ts_RMSE_3y,Prophet_RMSE_3y] #populate 'RMSE 3 year' column with RMSE_3y result for each model\n",
    "df_RMSE['RMSE (2 Year Testing Period)'] = [SARIMA_RMSE_2y,RF_RMSE_2y,LSTM_1ts_RMSE_2y,LSTM_4ts_RMSE_2y,Prophet_RMSE_2y] #populate 'RMSE 2 year' column with RMSE_3y result for each model\n",
    "df_RMSE['RMSE (Average)'] = (df_RMSE['RMSE (4 Year Testing Period)'] + df_RMSE['RMSE (3 Year Testing Period)'] + df_RMSE['RMSE (2 Year Testing Period)'])/3 #populate 'RMSE Average'column with average RMSE across the 3 training periods for each model\n",
    "df_RMSE = df_RMSE.set_index('Model') #set 'Model' column equal to the dataframe index\n",
    "df_RMSE.sort_values(by='RMSE (Average)', ascending=True) #sort RMSE results in ascending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84e005d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collate model predictions for 4 year test period\n",
    "All_Predictions_4y = df[['VALUE']].tail(16)\n",
    "All_Predictions_4y.rename(columns= {'VALUE':'Actual'},inplace=True)\n",
    "All_Predictions_4y['SARIMA'] = SARIMA_Predictions_4y.Predictions\n",
    "All_Predictions_4y['Random Forest'] = RF_Predictions_4y.Predictions\n",
    "All_Predictions_4y['LSTM (1 Timestep)'] = LSTM_1ts_Predictions_4y.Predictions\n",
    "All_Predictions_4y['LSTM (4 Timesteps)'] = LSTM_4ts_Predictions_4y.Predictions\n",
    "All_Predictions_4y['Prophet'] = Prophet_Predictions_4y.Predictions\n",
    "All_Predictions_4y = All_Predictions_4y.reset_index()\n",
    "\n",
    "#collate model predictions for 3 year test period\n",
    "All_Predictions_3y = df[['VALUE']].tail(12)\n",
    "All_Predictions_3y.rename(columns= {'VALUE':'Actual'},inplace=True)\n",
    "All_Predictions_3y['SARIMA'] = SARIMA_Predictions_3y.Predictions\n",
    "All_Predictions_3y['Random Forest'] = RF_Predictions_3y.Predictions\n",
    "All_Predictions_3y['LSTM (1 Timestep)'] = LSTM_1ts_Predictions_3y.Predictions\n",
    "All_Predictions_3y['LSTM (4 Timesteps)'] = LSTM_4ts_Predictions_3y.Predictions\n",
    "All_Predictions_3y['Prophet'] = Prophet_Predictions_3y.Predictions\n",
    "All_Predictions_3y = All_Predictions_3y.reset_index()\n",
    "\n",
    "#collate model predictions for 2 year test period\n",
    "All_Predictions_2y = df[['VALUE']].tail(8)\n",
    "All_Predictions_2y.rename(columns= {'VALUE':'Actual'},inplace=True)\n",
    "All_Predictions_2y['SARIMA'] = SARIMA_Predictions_2y.Predictions\n",
    "All_Predictions_2y['Random Forest'] = RF_Predictions_2y.Predictions\n",
    "All_Predictions_2y['LSTM (1 Timestep)'] = LSTM_1ts_Predictions_2y.Predictions\n",
    "All_Predictions_2y['LSTM (4 Timesteps)'] = LSTM_4ts_Predictions_2y.Predictions\n",
    "All_Predictions_2y['Prophet'] = Prophet_Predictions_2y.Predictions\n",
    "All_Predictions_2y = All_Predictions_2y.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1a4eda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06d3f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mitosheet import sheet\n",
    "sheet = sheet(All_Predictions_4y, analysis_to_replay=\"id-ojagjlwgkf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2578b003",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c8a1aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
